---
layout: post
title:  "Text Classficiation with DistilBERT (IMDB Dataset)"
date:   2024-02-20 22:31:09 +0800
category: [data_analysis, visualization, machine_learning, deep_learning]
tag: [numpy, pandas, seaborn, matplotlib, nlp, scikit-learn, pytorch, transfer_learning, classification]
summary: "In this notebook we will be exploring the IMDB dataset available on Kaggle, containing 50,000 reviews categorised as either positive or negative reviews. A text classification model will then be fine-tuned over DistilBERT and evaluated."
image: /images/banners/imdb.png
---

## Contents
1. [Overview](#overview)
2. [Data Preprocessing](#preprocess)
3. [Data Exploration](#exploration)
4. [Modelling](#bert)
5. [Conclusion](#conclusion)

***

<a id = 'overview'></a>
### 1. *Overview*
In this notebook we will be exploring the [IMDB dataset](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) available on Kaggle, containing 50,000 reviews categorised as either positive or negative reviews. A text classification model will then be fine-tuned over DistilBERT and evaluated.

***


```python
import pandas as pd
import numpy as np
import random
from collections import Counter
from datasets import load_dataset, Dataset

import seaborn as sns
import matplotlib.pyplot as plt

import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn import metrics, preprocessing
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, roc_curve

from transformers import AutoTokenizer, DistilBertTokenizerFast
from transformers import DataCollatorWithPadding
from transformers import AutoModelForSequenceClassification
from transformers import TrainingArguments, Trainer
import torch
import evaluate
from bs4 import BeautifulSoup
import lxml
```

    C:\Users\wenhao\anaconda3\envs\ML\Lib\site-packages\transformers\utils\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
      torch.utils._pytree._register_pytree_node(
    

***
<a id = 'preprocess'></a>
### 2. *Data Preprocessing*

We will begin by initialising certain variables that will be used over the course of this notebook, before importing the dataset using pandas.


```python
data_path = 'IMDB Dataset.csv' 
text_column_name = "review" 
label_column_name = "sentiment" 

model_name = "distilbert-base-uncased" 
test_size = 0.2 
num_labels = 2 

```


```python
df = pd.read_csv(data_path)
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>review</th>
      <th>sentiment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>One of the other reviewers has mentioned that ...</td>
      <td>positive</td>
    </tr>
    <tr>
      <th>1</th>
      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>
      <td>positive</td>
    </tr>
    <tr>
      <th>2</th>
      <td>I thought this was a wonderful way to spend ti...</td>
      <td>positive</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Basically there's a family where a little boy ...</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Petter Mattei's "Love in the Time of Money" is...</td>
      <td>positive</td>
    </tr>
  </tbody>
</table>
</div>




```python
df.isnull().sum()
```




    review       0
    sentiment    0
    dtype: int64




```python
df.sentiment.value_counts()
```




    sentiment
    positive    25000
    negative    25000
    Name: count, dtype: int64




```python
df['review'][1]
```




    'A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only "has got all the polari" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master\'s of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \'dream\' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell\'s murals decorating every surface) are terribly well done.'



We see that this is a balanced dataset with a 1:1 ratio of positive and negative reviews, hence there is no need to handle class imbalances.

Looking at the review we see that html tags exists in the dataset, we will remove them as part of data cleaning making use of BeautifulSoup package to parse our reviews. Additionally, we will also encode our "sentiment" column.


```python
class Cleaner():
  def __init__(self):
    pass
  def put_line_breaks(self,text):
    text = text.replace('</p>','</p>\n')
    return text
  def remove_html_tags(self,text):
    cleantext = BeautifulSoup(text, "lxml").text
    return cleantext
  def clean(self,text):
    text = self.put_line_breaks(text)
    text = self.remove_html_tags(text)
    return text
```


```python
cleaner = Cleaner()
df['text_cleaned'] = df[text_column_name].apply(cleaner.clean)
df.head()
```

    C:\Users\wenhao\AppData\Local\Temp\ipykernel_34920\4246655815.py:8: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.
      cleantext = BeautifulSoup(text, "lxml").text
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>review</th>
      <th>sentiment</th>
      <th>text_cleaned</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>One of the other reviewers has mentioned that ...</td>
      <td>positive</td>
      <td>One of the other reviewers has mentioned that ...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>
      <td>positive</td>
      <td>A wonderful little production. The filming tec...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>I thought this was a wonderful way to spend ti...</td>
      <td>positive</td>
      <td>I thought this was a wonderful way to spend ti...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Basically there's a family where a little boy ...</td>
      <td>negative</td>
      <td>Basically there's a family where a little boy ...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Petter Mattei's "Love in the Time of Money" is...</td>
      <td>positive</td>
      <td>Petter Mattei's "Love in the Time of Money" is...</td>
    </tr>
  </tbody>
</table>
</div>




```python
le = preprocessing.LabelEncoder()
le.fit(df[label_column_name].tolist())
df['label'] = le.transform(df[label_column_name].tolist())
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>review</th>
      <th>sentiment</th>
      <th>text_cleaned</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>One of the other reviewers has mentioned that ...</td>
      <td>positive</td>
      <td>One of the other reviewers has mentioned that ...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>
      <td>positive</td>
      <td>A wonderful little production. The filming tec...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>I thought this was a wonderful way to spend ti...</td>
      <td>positive</td>
      <td>I thought this was a wonderful way to spend ti...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Basically there's a family where a little boy ...</td>
      <td>negative</td>
      <td>Basically there's a family where a little boy ...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Petter Mattei's "Love in the Time of Money" is...</td>
      <td>positive</td>
      <td>Petter Mattei's "Love in the Time of Money" is...</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




```python
df['wordcount'] = df['text_cleaned'].str.split().str.len()
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>review</th>
      <th>sentiment</th>
      <th>text_cleaned</th>
      <th>label</th>
      <th>wordcount</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>One of the other reviewers has mentioned that ...</td>
      <td>positive</td>
      <td>One of the other reviewers has mentioned that ...</td>
      <td>1</td>
      <td>301</td>
    </tr>
    <tr>
      <th>1</th>
      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>
      <td>positive</td>
      <td>A wonderful little production. The filming tec...</td>
      <td>1</td>
      <td>156</td>
    </tr>
    <tr>
      <th>2</th>
      <td>I thought this was a wonderful way to spend ti...</td>
      <td>positive</td>
      <td>I thought this was a wonderful way to spend ti...</td>
      <td>1</td>
      <td>162</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Basically there's a family where a little boy ...</td>
      <td>negative</td>
      <td>Basically there's a family where a little boy ...</td>
      <td>0</td>
      <td>132</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Petter Mattei's "Love in the Time of Money" is...</td>
      <td>positive</td>
      <td>Petter Mattei's "Love in the Time of Money" is...</td>
      <td>1</td>
      <td>222</td>
    </tr>
  </tbody>
</table>
</div>




```python
df.wordcount.describe()
```




    count    50000.000000
    mean       227.114620
    std        168.278914
    min          4.000000
    25%        124.000000
    50%        170.000000
    75%        275.000000
    max       2450.000000
    Name: wordcount, dtype: float64



To help with the upcoming data exploration we have engineered a new feature to show the wordcount of each review entry, and a quick look shows most of the reviews have less than 275 words.

<a id='exploration'></a>
### 3. Data Exploration
Next, we will take a closer look at the distributions of wordcounts to investigate it they show any differences between positive and negative reviews.


```python
fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True)
sns.histplot(data=df.loc[df['sentiment']=='positive'], x='wordcount', ax=ax1, binwidth=50)
sns.histplot(data=df.loc[df['sentiment']=='negative'], x='wordcount', ax=ax2, binwidth=50)
fig.suptitle('Review Wordcount Distribution')
ax1.set_title('Positive reviews')
ax2.set_title('Negative reviews')
fig.tight_layout()
```

    C:\Users\wenhao\anaconda3\envs\ML\Lib\site-packages\seaborn\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    C:\Users\wenhao\anaconda3\envs\ML\Lib\site-packages\seaborn\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    


    
![png](output_15_1.png)
    



```python
fig, (ax1, ax2) = plt.subplots(1,2, sharey=True)
sns.boxplot(data=df.loc[df['sentiment']=='positive']['wordcount'].values, ax=ax1)
sns.boxplot(data=df.loc[df['sentiment']=='negative']['wordcount'].values, ax=ax2)
fig.suptitle('Review Wordcount Boxplot')
ax1.set_title('Positive reviews')
ax2.set_title('Negative reviews')
fig.tight_layout()
```


    
![png](output_16_0.png)
    


Looking at the above histograms and boxplots of wordcounts for Positive and Negative reviews, we see that the distributions are almost the same, except for a few outliers under our positive reviews.

Next we will look at some of the most common words and phrases within the dataset.


```python
def most_common_words(df, n):
    corpus = []
    stop = set(stopwords.words('english'))
    for review in df.text_cleaned:
        for word in review.split():
            if word.strip().lower() not in stop and word.strip().lower().isalpha():
                corpus.append(word.strip())
    
    counter_words = Counter(corpus).most_common(n)
    counter_words = dict(counter_words)
    tmp = pd.DataFrame(columns = ["Word", 'Count'])
    tmp["Word"] = list(counter_words.keys())
    tmp['Count'] = list(counter_words.values())
    return tmp

# def most_common_ngrams(corpus, n, gram):
#     vec = CountVectorizer(ngram_range = (gram, gram)).fit(corpus)
#     bow = vec.transform(corpus)
#     word_sum = bow.sum(axis=0)
#     word_freq = [(word, word_sum[0, idx]) for word, idx in vec.vocabulary_.items()]
#     word_freq = sorted(word_freq, key = lambda x: x[1], reverse = True)
#     return word_freq[:n]

def most_common_ngrams(df, n, gram, name):
    corpus = []
    stop = set(stopwords.words('english'))
    for review in df.text_cleaned:
        words = review.split()
        words = [word for word in words if word not in stop]
        for i in range(len(words)-gram+1):
            corpus.append(' '.join(words[i:i+gram]))
    counter_ngrams = Counter(corpus).most_common(n)
    ngrams = dict(counter_ngrams)
    tmp = pd.DataFrame(columns = [str(name), 'Count'])
    tmp[str(name)] = list(ngrams.keys())
    tmp['Count'] = list(ngrams.values())
    return tmp
```


```python
positive_corpus = most_common_words(df.loc[df['sentiment']=='positive'], 10)
negative_corpus = most_common_words(df.loc[df['sentiment']=='negative'], 10)

positive_bigram = most_common_ngrams(df.loc[df['sentiment']=='positive'], 10, 2, 'Bigram')
negative_bigram = most_common_ngrams(df.loc[df['sentiment']=='negative'], 10, 2, 'Bigram')
positive_trigram = most_common_ngrams(df.loc[df['sentiment']=='positive'], 10, 3, 'Trigram')
negative_trigram = most_common_ngrams(df.loc[df['sentiment']=='negative'], 10, 3, 'Trigram')
```


```python
fig, (ax1, ax2) = plt.subplots(1,2)
sns.barplot(data=positive_corpus, x='Count', y='Word', ax=ax1, palette = 'Paired')
sns.barplot(data=negative_corpus, x='Count', y='Word', ax=ax2, palette = 'Paired')
fig.suptitle('10 most common words in reviews')
ax1.set_title('Positive Review')
ax2.set_title('Negative Review')
plt.tight_layout()
```


    
![png](output_20_0.png)
    



```python
fig, (ax1, ax2) = plt.subplots(1,2)
sns.barplot(data=positive_bigram, x='Count', y='Bigram', ax=ax1, palette = 'Paired')
sns.barplot(data=negative_bigram, x='Count', y='Bigram', ax=ax2, palette = 'Paired')
fig.suptitle('10 most common bigrams in reviews')
ax1.set_title('Positive Review')
ax2.set_title('Negative Review')
plt.tight_layout()
```


    
![png](output_21_0.png)
    



```python
fig, (ax1, ax2) = plt.subplots(1,2)
sns.barplot(data=positive_trigram, x='Count', y='Trigram', ax=ax1, palette = 'Paired')
sns.barplot(data=negative_trigram, x='Count', y='Trigram', ax=ax2, palette = 'Paired')
fig.suptitle('10 most common trigrams in reviews')
ax1.set_title('Positive Review')
ax2.set_title('Negative Review')
plt.tight_layout()
```


    
![png](output_22_0.png)
    


We are able to see phrases such as "I highly recommend" and "I would recommend" appearing in positive reviews giving a positive connotation while phrases like "one worst movies" and "worst movie I" appear in negative reviews giving a negative connotation. However, many of the most common words and phrases appear in both positive and negative reviews, suggesting that it would be better to focus on the exact phrases used rather than how frequently they are used.

For the purpose of data exploration, the above plots were created after removing stopwords from our dataset, in the next section where we fine tune a BERT model for text classification stopwords will be left in the dataset to provide context clues for our model.

***

<a id = 'bert'></a>
### 4. Modelling (BERT)
In this section we will fine tune a BERT model for text classification, due to limited computation power, we will be sampling 10% of the original dataset to use for training and validation, and another 5% as our holdout set for model evaluation after training is completed.


```python
# Reduce dataset to 5000 rows maintaining 1:1 ratio of review sentiments, shuffle and reset index
data = df.groupby('sentiment').apply(lambda x: x.sample(frac=0.15)).droplevel('sentiment')
holdout = pd.concat([data.iloc[:1250],data.iloc[6250:]]).sample(frac=1)
holdout = holdout.rename(columns={'label': 'true_label'})
holdout
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>review</th>
      <th>sentiment</th>
      <th>text_cleaned</th>
      <th>true_label</th>
      <th>wordcount</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>16369</th>
      <td>Oh, this is such a glorious musical. There's a...</td>
      <td>positive</td>
      <td>Oh, this is such a glorious musical. There's a...</td>
      <td>1</td>
      <td>121</td>
    </tr>
    <tr>
      <th>11852</th>
      <td>As an old white housewife I can still apprecia...</td>
      <td>positive</td>
      <td>As an old white housewife I can still apprecia...</td>
      <td>1</td>
      <td>114</td>
    </tr>
    <tr>
      <th>49783</th>
      <td>Saw this movie at the Rotterdam IFF. You may q...</td>
      <td>positive</td>
      <td>Saw this movie at the Rotterdam IFF. You may q...</td>
      <td>1</td>
      <td>54</td>
    </tr>
    <tr>
      <th>21792</th>
      <td>The minutiae of what's involved in carrying ou...</td>
      <td>positive</td>
      <td>The minutiae of what's involved in carrying ou...</td>
      <td>1</td>
      <td>152</td>
    </tr>
    <tr>
      <th>21757</th>
      <td>Greetings again from the darkness. What a reli...</td>
      <td>positive</td>
      <td>Greetings again from the darkness. What a reli...</td>
      <td>1</td>
      <td>271</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>17962</th>
      <td>I'm glad I rented this movie for one reason: i...</td>
      <td>negative</td>
      <td>I'm glad I rented this movie for one reason: i...</td>
      <td>0</td>
      <td>148</td>
    </tr>
    <tr>
      <th>34361</th>
      <td>Overall, a well done movie. There were the par...</td>
      <td>positive</td>
      <td>Overall, a well done movie. There were the par...</td>
      <td>1</td>
      <td>349</td>
    </tr>
    <tr>
      <th>3722</th>
      <td>Doesn't anyone bother to check where this kind...</td>
      <td>negative</td>
      <td>Doesn't anyone bother to check where this kind...</td>
      <td>0</td>
      <td>200</td>
    </tr>
    <tr>
      <th>38341</th>
      <td>Such a long awaited movie.. But it has disappo...</td>
      <td>negative</td>
      <td>Such a long awaited movie.. But it has disappo...</td>
      <td>0</td>
      <td>206</td>
    </tr>
    <tr>
      <th>49517</th>
      <td>This movie is a terrible attempt at a spoof. I...</td>
      <td>negative</td>
      <td>This movie is a terrible attempt at a spoof. I...</td>
      <td>0</td>
      <td>144</td>
    </tr>
  </tbody>
</table>
<p>2500 rows × 5 columns</p>
</div>




```python
data = data.iloc[1250:6250].sample(frac=1)
data
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>review</th>
      <th>sentiment</th>
      <th>text_cleaned</th>
      <th>label</th>
      <th>wordcount</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>29963</th>
      <td>This is Peter Falk's film. Period.&lt;br /&gt;&lt;br /&gt;...</td>
      <td>negative</td>
      <td>This is Peter Falk's film. Period.I was 10 yea...</td>
      <td>0</td>
      <td>246</td>
    </tr>
    <tr>
      <th>35201</th>
      <td>I don't see that much wrong with this movie. G...</td>
      <td>positive</td>
      <td>I don't see that much wrong with this movie. G...</td>
      <td>1</td>
      <td>218</td>
    </tr>
    <tr>
      <th>25325</th>
      <td>This movie wasn't that bad when compared to th...</td>
      <td>negative</td>
      <td>This movie wasn't that bad when compared to th...</td>
      <td>0</td>
      <td>151</td>
    </tr>
    <tr>
      <th>44489</th>
      <td>There is no greater disservice to do to histor...</td>
      <td>positive</td>
      <td>There is no greater disservice to do to histor...</td>
      <td>1</td>
      <td>538</td>
    </tr>
    <tr>
      <th>7754</th>
      <td>Posh Spice Victoria Beckham and her alleged ne...</td>
      <td>negative</td>
      <td>Posh Spice Victoria Beckham and her alleged ne...</td>
      <td>0</td>
      <td>657</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>44529</th>
      <td>OK from the point of view of an American, who ...</td>
      <td>negative</td>
      <td>OK from the point of view of an American, who ...</td>
      <td>0</td>
      <td>189</td>
    </tr>
    <tr>
      <th>11739</th>
      <td>I love bad movies: Showgirls, Plan 9 from Oute...</td>
      <td>negative</td>
      <td>I love bad movies: Showgirls, Plan 9 from Oute...</td>
      <td>0</td>
      <td>122</td>
    </tr>
    <tr>
      <th>3805</th>
      <td>It was probably just my DVD---but I would not ...</td>
      <td>negative</td>
      <td>It was probably just my DVD---but I would not ...</td>
      <td>0</td>
      <td>185</td>
    </tr>
    <tr>
      <th>5756</th>
      <td>For some unknown reason, 7 years ago, I watche...</td>
      <td>negative</td>
      <td>For some unknown reason, 7 years ago, I watche...</td>
      <td>0</td>
      <td>127</td>
    </tr>
    <tr>
      <th>37916</th>
      <td>whomever thought of having sequels to Iron Eag...</td>
      <td>negative</td>
      <td>whomever thought of having sequels to Iron Eag...</td>
      <td>0</td>
      <td>219</td>
    </tr>
  </tbody>
</table>
<p>5000 rows × 5 columns</p>
</div>




```python
# train / validation dataset splits
df_train, df_test = train_test_split(data[['text_cleaned','label']], test_size=test_size)
```


```python
# converting out pandas dataframes to pytorch datasets
train_dataset = Dataset.from_pandas(df_train)
test_dataset = Dataset.from_pandas(df_test)
```


```python
# tokenizer to convert our words to tokens
tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)

def preprocess_function(examples):
    return tokenizer(examples["text_cleaned"], truncation=True)

tokenized_train = train_dataset.map(preprocess_function, batched=True)
tokenized_test = test_dataset.map(preprocess_function, batched=True)
```


    Map:   0%|          | 0/4000 [00:00<?, ? examples/s]



    Map:   0%|          | 0/1000 [00:00<?, ? examples/s]



```python
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)
```

    Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']
    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
    


```python
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```


```python
metric = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```


```python
training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=5,
    weight_decay=0.01,
    evaluation_strategy = "epoch",
    logging_strategy="epoch"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
    
)
```


```python
trainer.train()
```



    <div>

      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [1250/1250 08:39, Epoch 5/5]
    </div>
    <table border="1" class="dataframe">
  <thead>
 <tr style="text-align: left;">
      <th>Epoch</th>
      <th>Training Loss</th>
      <th>Validation Loss</th>
      <th>Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>0.359300</td>
      <td>0.253881</td>
      <td>0.901000</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.176800</td>
      <td>0.296619</td>
      <td>0.906000</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.095000</td>
      <td>0.434078</td>
      <td>0.895000</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.041500</td>
      <td>0.437259</td>
      <td>0.906000</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.019900</td>
      <td>0.460906</td>
      <td>0.904000</td>
    </tr>
  </tbody>
</table><p>





    TrainOutput(global_step=1250, training_loss=0.13849494819641114, metrics={'train_runtime': 520.6204, 'train_samples_per_second': 38.416, 'train_steps_per_second': 2.401, 'total_flos': 2603187224294592.0, 'train_loss': 0.13849494819641114, 'epoch': 5.0})



It appears that validation loss has increased after the first epoch while training loss continued to decrease, suggesting some overfitting happening in this case. However, looking at validation accuracy we see that the performance of the remained constant through all 5 epochs.

We will see if this performance holds true for our holdout set, extracting the model's predictions (label and score) from our pipeline.


```python
from transformers import pipeline
classifier = pipeline(
    task='text-classification',
    model=model,
    tokenizer=tokenizer,
    device=0,
    truncation=True,
    batch_size=8
)
```


```python
holdout['result'] = holdout['text_cleaned'].apply(lambda x: classifier(x))
holdout['sentiment'] = holdout['result'].str[0].str['label']
holdout['score'] = holdout['result'].str[0].str['score']
holdout
```

    C:\Users\wenhao\anaconda3\envs\ML\Lib\site-packages\transformers\pipelines\base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
      warnings.warn(
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>review</th>
      <th>sentiment</th>
      <th>text_cleaned</th>
      <th>true_label</th>
      <th>wordcount</th>
      <th>result</th>
      <th>score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>16369</th>
      <td>Oh, this is such a glorious musical. There's a...</td>
      <td>LABEL_1</td>
      <td>Oh, this is such a glorious musical. There's a...</td>
      <td>1</td>
      <td>121</td>
      <td>[{'label': 'LABEL_1', 'score': 0.9983893632888...</td>
      <td>0.998389</td>
    </tr>
    <tr>
      <th>11852</th>
      <td>As an old white housewife I can still apprecia...</td>
      <td>LABEL_1</td>
      <td>As an old white housewife I can still apprecia...</td>
      <td>1</td>
      <td>114</td>
      <td>[{'label': 'LABEL_1', 'score': 0.9985773563385...</td>
      <td>0.998577</td>
    </tr>
    <tr>
      <th>49783</th>
      <td>Saw this movie at the Rotterdam IFF. You may q...</td>
      <td>LABEL_0</td>
      <td>Saw this movie at the Rotterdam IFF. You may q...</td>
      <td>1</td>
      <td>54</td>
      <td>[{'label': 'LABEL_0', 'score': 0.7969191670417...</td>
      <td>0.796919</td>
    </tr>
    <tr>
      <th>21792</th>
      <td>The minutiae of what's involved in carrying ou...</td>
      <td>LABEL_1</td>
      <td>The minutiae of what's involved in carrying ou...</td>
      <td>1</td>
      <td>152</td>
      <td>[{'label': 'LABEL_1', 'score': 0.9989652633666...</td>
      <td>0.998965</td>
    </tr>
    <tr>
      <th>21757</th>
      <td>Greetings again from the darkness. What a reli...</td>
      <td>LABEL_1</td>
      <td>Greetings again from the darkness. What a reli...</td>
      <td>1</td>
      <td>271</td>
      <td>[{'label': 'LABEL_1', 'score': 0.9985762834548...</td>
      <td>0.998576</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>17962</th>
      <td>I'm glad I rented this movie for one reason: i...</td>
      <td>LABEL_0</td>
      <td>I'm glad I rented this movie for one reason: i...</td>
      <td>0</td>
      <td>148</td>
      <td>[{'label': 'LABEL_0', 'score': 0.9990332126617...</td>
      <td>0.999033</td>
    </tr>
    <tr>
      <th>34361</th>
      <td>Overall, a well done movie. There were the par...</td>
      <td>LABEL_0</td>
      <td>Overall, a well done movie. There were the par...</td>
      <td>1</td>
      <td>349</td>
      <td>[{'label': 'LABEL_0', 'score': 0.9182419180870...</td>
      <td>0.918242</td>
    </tr>
    <tr>
      <th>3722</th>
      <td>Doesn't anyone bother to check where this kind...</td>
      <td>LABEL_0</td>
      <td>Doesn't anyone bother to check where this kind...</td>
      <td>0</td>
      <td>200</td>
      <td>[{'label': 'LABEL_0', 'score': 0.9989865422248...</td>
      <td>0.998987</td>
    </tr>
    <tr>
      <th>38341</th>
      <td>Such a long awaited movie.. But it has disappo...</td>
      <td>LABEL_0</td>
      <td>Such a long awaited movie.. But it has disappo...</td>
      <td>0</td>
      <td>206</td>
      <td>[{'label': 'LABEL_0', 'score': 0.9987562894821...</td>
      <td>0.998756</td>
    </tr>
    <tr>
      <th>49517</th>
      <td>This movie is a terrible attempt at a spoof. I...</td>
      <td>LABEL_0</td>
      <td>This movie is a terrible attempt at a spoof. I...</td>
      <td>0</td>
      <td>144</td>
      <td>[{'label': 'LABEL_0', 'score': 0.9990750551223...</td>
      <td>0.999075</td>
    </tr>
  </tbody>
</table>
<p>2500 rows × 7 columns</p>
</div>




```python
holdout['sentiment']= holdout['sentiment'].map({'LABEL_1':1, 'LABEL_0':0})
```


```python
holdout
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>review</th>
      <th>sentiment</th>
      <th>text_cleaned</th>
      <th>true_label</th>
      <th>wordcount</th>
      <th>result</th>
      <th>score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>16369</th>
      <td>Oh, this is such a glorious musical. There's a...</td>
      <td>1</td>
      <td>Oh, this is such a glorious musical. There's a...</td>
      <td>1</td>
      <td>121</td>
      <td>[{'label': 'LABEL_1', 'score': 0.9983893632888...</td>
      <td>0.998389</td>
    </tr>
    <tr>
      <th>11852</th>
      <td>As an old white housewife I can still apprecia...</td>
      <td>1</td>
      <td>As an old white housewife I can still apprecia...</td>
      <td>1</td>
      <td>114</td>
      <td>[{'label': 'LABEL_1', 'score': 0.9985773563385...</td>
      <td>0.998577</td>
    </tr>
    <tr>
      <th>49783</th>
      <td>Saw this movie at the Rotterdam IFF. You may q...</td>
      <td>0</td>
      <td>Saw this movie at the Rotterdam IFF. You may q...</td>
      <td>1</td>
      <td>54</td>
      <td>[{'label': 'LABEL_0', 'score': 0.7969191670417...</td>
      <td>0.796919</td>
    </tr>
    <tr>
      <th>21792</th>
      <td>The minutiae of what's involved in carrying ou...</td>
      <td>1</td>
      <td>The minutiae of what's involved in carrying ou...</td>
      <td>1</td>
      <td>152</td>
      <td>[{'label': 'LABEL_1', 'score': 0.9989652633666...</td>
      <td>0.998965</td>
    </tr>
    <tr>
      <th>21757</th>
      <td>Greetings again from the darkness. What a reli...</td>
      <td>1</td>
      <td>Greetings again from the darkness. What a reli...</td>
      <td>1</td>
      <td>271</td>
      <td>[{'label': 'LABEL_1', 'score': 0.9985762834548...</td>
      <td>0.998576</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>17962</th>
      <td>I'm glad I rented this movie for one reason: i...</td>
      <td>0</td>
      <td>I'm glad I rented this movie for one reason: i...</td>
      <td>0</td>
      <td>148</td>
      <td>[{'label': 'LABEL_0', 'score': 0.9990332126617...</td>
      <td>0.999033</td>
    </tr>
    <tr>
      <th>34361</th>
      <td>Overall, a well done movie. There were the par...</td>
      <td>0</td>
      <td>Overall, a well done movie. There were the par...</td>
      <td>1</td>
      <td>349</td>
      <td>[{'label': 'LABEL_0', 'score': 0.9182419180870...</td>
      <td>0.918242</td>
    </tr>
    <tr>
      <th>3722</th>
      <td>Doesn't anyone bother to check where this kind...</td>
      <td>0</td>
      <td>Doesn't anyone bother to check where this kind...</td>
      <td>0</td>
      <td>200</td>
      <td>[{'label': 'LABEL_0', 'score': 0.9989865422248...</td>
      <td>0.998987</td>
    </tr>
    <tr>
      <th>38341</th>
      <td>Such a long awaited movie.. But it has disappo...</td>
      <td>0</td>
      <td>Such a long awaited movie.. But it has disappo...</td>
      <td>0</td>
      <td>206</td>
      <td>[{'label': 'LABEL_0', 'score': 0.9987562894821...</td>
      <td>0.998756</td>
    </tr>
    <tr>
      <th>49517</th>
      <td>This movie is a terrible attempt at a spoof. I...</td>
      <td>0</td>
      <td>This movie is a terrible attempt at a spoof. I...</td>
      <td>0</td>
      <td>144</td>
      <td>[{'label': 'LABEL_0', 'score': 0.9990750551223...</td>
      <td>0.999075</td>
    </tr>
  </tbody>
</table>
<p>2500 rows × 7 columns</p>
</div>




```python
print('Accuracy: ', accuracy_score(holdout['true_label'], holdout['sentiment']))
print('F1: ', f1_score(holdout['true_label'], holdout['sentiment']))
print('Confusion Matrix: ', confusion_matrix(holdout['true_label'], holdout['sentiment']))
```

    Accuracy:  0.9044
    F1:  0.9051210797935689
    Confusion Matrix:  [[1121  129]
     [ 110 1140]]
    

Predicting on our holdout set, we see that the model achieves approximately 90.44% accuracy and an F1 score of 0.9051. This is consistent with our validation accuracy while training, suggesting that the model will perform well on similar reviews that are unseen.

***

<a id='conclusion'></a>
### 5. Conclusion
In this notebook we have explored the IMDB Movie Reviews dataset using some techniques commonly found in NLP and sentiment analysis. We have also fine tuned a text classificaiton model on DistilBert, achieving a good performance of ~90% accuracy on unseen dataset.

---
layout: post
title:  "Image Classification with Convolutional Neural Network (CNN)"
date:   2024-01-01 22:31:09 +0800
category: [data_wrangling, data_analysis, visualization, deep_learning]
tag: [numpy, pandas, seaborn, matplotlib, cv2, computer_vision, classification, tensorflow]
summary: "In this notebook we train classification models to identify the activities and subjects from a smartphone sensor dataset."
image: /images/banners/cnn.png
---

## Contents
1. [Overview](#1)
2. [Importing required libraries](#2)
3. [Dataset](#3)
4. [Machine learning modelling](#4)<br>
    4.1 [Data preprocessing](#4.1)<br>
    4.2 [Model training](#4.2)<br>
    4.3 [Model evaluation](#4.3)<br>
    4.4 [Examples](#4.4)<br>
5. [Conclusion](#5)

***

<a id = '1'></a>
## 1. *Overview*
This notebook shows how a CNN can learn to recognise the presence of certain features in different images (gender in this case). Potentially, image recognition can be applied to solve many different problems, such as object detection, face detection, security surveillance, among many others.

***

<a id = '2'></a>
## 2. *Importing required libraries*


```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import f1_score, accuracy_score

from keras import optimizers
from keras.callbacks import ModelCheckpoint

import tensorflow as tf
from tensorflow.keras.applications import InceptionV3
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Dropout, Conv2D, MaxPool2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, array_to_img, img_to_array
%matplotlib inline
```

***
<a id = '3'></a>
## 3. *Dataset exploration*
For this project we will use <a href = "https://www.kaggle.com/jessicali9530/celeba-dataset" target="_blank">CelebFaces Attributes (CelebA)</a> dataset found on Kaggle

<blockquote>Context

A popular component of computer vision and deep learning revolves around identifying faces for various applications from logging into your phone with your face or searching through surveillance images for a particular suspect. This dataset is great for training and testing models for face detection, particularly for recognising facial attributes such as finding people with brown hair, are smiling, or wearing glasses. Images cover large pose variations, background clutter, diverse people, supported by a large quantity of images and rich annotations. This data was originally collected by researchers at MMLAB, The Chinese University of Hong Kong (specific reference in Acknowledgment section).
Content

Overall

    202,599 number of face images of various celebrities
    10,177 unique identities, but names of identities are not given
    40 binary attribute annotations per image
    5 landmark locations

Data Files

    imgalignceleba.zip: All the face images, cropped and aligned
    listevalpartition.csv: Recommended partitioning of images into training, validation, testing sets. Images 1-162770 are training, 162771-182637 are validation, 182638-202599 are testing
    listbboxceleba.csv: Bounding box information for each image. "x1" and "y1" represent the upper left point coordinate of bounding box. "width" and "height" represent the width and height of bounding box
    listlandmarksalign_celeba.csv: Image landmarks and their respective coordinates. There are 5 landmarks: left eye, right eye, nose, left mouth, right mouth
    listattrceleba.csv: Attribute labels for each image. There are 40 attributes. "1" represents positive while "-1" represents negative

Acknowledgements

Original data and banner image source came from http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html
As mentioned on the website, the CelebA dataset is available for non-commercial research purposes only. For specifics please refer to the website.

The creators of this dataset wrote the following paper employing CelebA for face detection:

S. Yang, P. Luo, C. C. Loy, and X. Tang, "From Facial Parts Responses to Face Detection: A Deep Learning Approach", in IEEE International Conference on Computer Vision (ICCV), 2015</blockquote>

#### Format of dataset
The dataset contains 202,599 images all within a single folder, with accompanying csv files containing labels for their attributes (facial features, gender, etc.) and partition (train, validation, test). In the interest of time, a random subsample of the full dataset will be used for this notebook. The [accompanying .py file]() is the script used to separate the sampled images into their respective train/validation/test folders.

Images within the dataset are all of resolution 178x218. An example image is shown below.


```python
# Folder containing dataset
main_folder = 'E:\\datascience\\celebA\\'
train_folder = main_folder + 'training\\'
validation_folder = main_folder + 'validation\\'
test_folder = main_folder + 'test\\'

# Example image
EXAMPLE_PIC = test_folder + 'test_Male\\197126.jpg'
img = load_img(EXAMPLE_PIC)
plt.grid(False)
plt.imshow(img)
```




    <matplotlib.image.AxesImage at 0x1a43e2db5b0>




    
![png](/images/cnn_image/output_3_1.png)
    



```python
# Load attributes of images
df_attr = pd.read_csv(main_folder + 'list_attr_celeba.csv')
df_attr.replace(to_replace = -1, value = 0, inplace = True)

# Print all available attributes in dataset
print("List of Features: ")
for i, j in enumerate(df_attr.columns):
    print(i, j)

print('#'*20)

# Print selected attribute of the example image above
print(df_attr.loc[df_attr['image_id'] == EXAMPLE_PIC.split('\\')[-1]][['Male']])
```

    List of Features: 
    0 image_id
    1 5_o_Clock_Shadow
    2 Arched_Eyebrows
    3 Attractive
    4 Bags_Under_Eyes
    5 Bald
    6 Bangs
    7 Big_Lips
    8 Big_Nose
    9 Black_Hair
    10 Blond_Hair
    11 Blurry
    12 Brown_Hair
    13 Bushy_Eyebrows
    14 Chubby
    15 Double_Chin
    16 Eyeglasses
    17 Goatee
    18 Gray_Hair
    19 Heavy_Makeup
    20 High_Cheekbones
    21 Male
    22 Mouth_Slightly_Open
    23 Mustache
    24 Narrow_Eyes
    25 No_Beard
    26 Oval_Face
    27 Pale_Skin
    28 Pointy_Nose
    29 Receding_Hairline
    30 Rosy_Cheeks
    31 Sideburns
    32 Smiling
    33 Straight_Hair
    34 Wavy_Hair
    35 Wearing_Earrings
    36 Wearing_Hat
    37 Wearing_Lipstick
    38 Wearing_Necklace
    39 Wearing_Necktie
    40 Young
    ####################
            Male
    197125     0
    

Above we see a list of the labeled features available for each image of the dataset, and the "Male" class we will be modelling for. The example image above is labeled '0' for "Male" meaning that it is not a male photographed.

#### Distribution of Dataset
CNNs have been shown to perform best when the predicted classes are balanced. In this dataset we can observe that the chosen attributes/classes are imbalanced, suggesting that we will need to balance the classes when modelling. Previously when subsampling the dataset class imbalance was already considered and stratified sampling was conducted.


```python
# Plot Male vs Female
plt.title('Original Distribution of "Male" class')
sns.countplot(y= 'Male', data = df_attr)
plt.show() # More female than male in dataset
print('Number of Female:',len(df_attr[df_attr['Male'] == 0]))
print('Number of Male:',len(df_attr[df_attr['Male'] == 1]))

# Snippet of code from accompanying .py file
training_id = pd.concat([df_attr.loc[(df_attr.Male == 0)].sample(20000//2),
                        df_attr.loc[(df_attr.Male == 1)].sample(20000//2)])

# Plot training dataset Male vs Female
plt.title('Stratified Sampling Distribution of "Male" class')
sns.countplot(y= 'Male', data = training_id)
plt.show() # More female than male in dataset
print('Number of Female:',len(training_id[training_id['Male'] == 0]))
print('Number of Male:',len(training_id[training_id['Male'] == 1]))
```


    
![png](/images/cnn_image/output_6_0.png)
    


    Number of Female: 118165
    Number of Male: 84434
    


    
![png](/images/cnn_image/output_6_2.png)
    


    Number of Female: 10000
    Number of Male: 10000
    

***

<a id = '4'></a>
<a id = '4.1'></a>
## 4. *Machine learning modelling*
### 4.1 *Data preprocessing*
#### Splitting the dataset into training, validation and test sets
As quoted from the description of the dataset, recommended partitioning of dataset
<blockquote>listevalpartition.csv: Recommended partitioning of images into training, validation, testing sets. Images 1-162770 are training, 162771-182637 are validation, 182638-202599 are testing</blockquote>
Taking training time into consideration however, the number of images used will be reduced to 20000, 5000, 5000 for training, validation, and test sets respectively.


```python
# Initialise variables
EPOCHS = 50
LEARNING_RATE = 1e-3
BATCH_SIZE = 128

WIDTH = 178
HEIGHT = 218
TARGET_SIZE = (HEIGHT, WIDTH)
```

#### Data augmentation
This step generates additional images with modifications made to the images available in the dataset, allowing the model to learn from this new images to better predict for never seen images when the model is deployed. Shown below is an example of what data augmentation will do to existing images within the dataset.


```python
# Reshape example image for keras
img = img_to_array(img)
img = img.reshape(1, HEIGHT, WIDTH, 3)
print("Image shape: ", img.shape)

# Image generator for data augmentation of training set
train_datagen = ImageDataGenerator(horizontal_flip = True,
                                   width_shift_range = 0.4,
                                   height_shift_range = 0.4,
                                   zoom_range = 0.3,
                                   rotation_range = 20,
                                   rescale = 1./255.
                                   )

# Plotting augmented images of the example image
plt.figure(figsize = (20,10))
plt.title('Data Augmentation', fontsize = 28)

i = 0
for batch in train_datagen.flow(img, batch_size = 1):
    plt.subplot(3, 5, i+1)
    plt.grid(False)
    plt.imshow(batch.reshape(218, 178, 3))
    
    if i == 9:
        break
    else:
        i += 1
plt.show()
```

    Image shape:  (1, 218, 178, 3)
    


    
![png](/images/cnn_image/output_10_1.png)
    



```python
# Data generator for validation and test sets (No augmentation other than scaling RGB values between 0 and 1)
test_gen = ImageDataGenerator(rescale = 1./255.)
```


```python
# Training set data generator for model fitting
train_generator = train_datagen.flow_from_directory(train_folder,
                                 target_size = TARGET_SIZE,
                                 batch_size = BATCH_SIZE,
                                 class_mode = 'binary'
                                 )
```

    Found 20000 images belonging to 2 classes.
    


```python
# Validation set data generator 
valid_generator = test_gen.flow_from_directory(validation_folder,
                                 target_size = TARGET_SIZE,
                                 batch_size = BATCH_SIZE,
                                 class_mode = 'binary'
                                 )
```

    Found 5000 images belonging to 2 classes.
    


```python
# Test set data generator
test_generator = test_gen.flow_from_directory(test_folder,
                                 target_size = TARGET_SIZE,
                                 batch_size = BATCH_SIZE,
                                 class_mode = 'binary',
                                 shuffle = False
                                 )
```

    Found 5000 images belonging to 1 classes.
    


```python
# Check class labels
print("Train: \n",train_generator.class_indices)
print("Validation: \n",valid_generator.class_indices)
print("Test: \n",test_generator.class_indices)
```

    Train: 
     {'Male': 0, 'NotMale': 1}
    Validation: 
     {'Male': 0, 'NotMale': 1}
    Test: 
     {'test_Male': 0}
    

Now we have our data generators for all three partitions, and have confirmed that the generators recognised the two classes for the training and validation set, while test set class is unknown.

<a id = '4.2'></a>
### 4.2 *Model Training*
#### Building the model
In this project InceptionV3 is a pre-trained model selected for transfer learning. The top layers are removed and replaced by new layers that classifies the desired labels. The pre-trained model has been trained on the ImageNet dataset, learning to distinguish features such as textures, edges, contrasts, among others in the lower layers. Hence, retraining of the lower layers are not required


```python
# InceptionV3 model
model = Sequential()
model.add(InceptionV3(include_top = False, pooling = 'avg', weights = 'imagenet'))

# Add custom top layers that will be trained
model.add(Flatten())
model.add(BatchNormalization())

model.add(Dense(2048, activation = 'relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))

model.add(Dense(1024, activation = 'relu'))
model.add(BatchNormalization())

model.add(Dense(1, activation = 'sigmoid'))

# InceptionV3 layers will not be trained
model.layers[0].trainable = False
```


```python
model.summary()
```

    Model: "sequential"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    inception_v3 (Functional)    (None, 2048)              21802784  
    _________________________________________________________________
    flatten (Flatten)            (None, 2048)              0         
    _________________________________________________________________
    batch_normalization_94 (Batc (None, 2048)              8192      
    _________________________________________________________________
    dense (Dense)                (None, 2048)              4196352   
    _________________________________________________________________
    batch_normalization_95 (Batc (None, 2048)              8192      
    _________________________________________________________________
    dropout (Dropout)            (None, 2048)              0         
    _________________________________________________________________
    dense_1 (Dense)              (None, 1024)              2098176   
    _________________________________________________________________
    batch_normalization_96 (Batc (None, 1024)              4096      
    _________________________________________________________________
    dense_2 (Dense)              (None, 1)                 1025      
    =================================================================
    Total params: 28,118,817
    Trainable params: 6,305,793
    Non-trainable params: 21,813,024
    _________________________________________________________________
    


```python
# Compile the model
model.compile(optimizer = 'adam',
              loss = 'binary_crossentropy',
              metrics = ['accuracy'])
```

#### Training the model
The model will be trained to identify "Male" attributes in images. A check pointer will be used to save the best performing models during the training process.


```python
# Training the model
checkpointer_male = ModelCheckpoint(filepath = 'saved_models\\weights.best.inc.male.hdf5')

hist_male = model.fit(train_generator,
                     steps_per_epoch = len(train_generator.filenames)//BATCH_SIZE,
                     epochs = EPOCHS,
                     validation_data = valid_generator,
                     validation_steps = len(valid_generator.filenames)//BATCH_SIZE,
                     callbacks = [checkpointer_male]
                     )

# Plot loss function value through epochs
plt.figure(figsize=(12, 4))
plt.plot(hist_male.history['loss'], label = 'training')
plt.plot(hist_male.history['val_loss'], label = 'validation')
plt.legend()
plt.title('Loss')
plt.show()


# Plot accuracy through epochs
plt.figure(figsize=(12, 4))
plt.plot(hist_male.history['accuracy'], label = 'training')
plt.plot(hist_male.history['val_accuracy'], label = 'validation')
plt.legend()
plt.title('Accuracy')
plt.show()
```

    Epoch 1/50
    156/156 [==============================] - 449s 3s/step - loss: 0.3897 - accuracy: 0.8575 - val_loss: 0.1958 - val_accuracy: 0.9235
    Epoch 2/50
    156/156 [==============================] - 151s 966ms/step - loss: 0.2856 - accuracy: 0.8814 - val_loss: 0.1815 - val_accuracy: 0.9273
    Epoch 3/50
    156/156 [==============================] - 151s 967ms/step - loss: 0.2677 - accuracy: 0.8856 - val_loss: 0.1932 - val_accuracy: 0.9273
    Epoch 4/50
    156/156 [==============================] - 151s 966ms/step - loss: 0.2658 - accuracy: 0.8884 - val_loss: 0.1796 - val_accuracy: 0.9313
    Epoch 5/50
    156/156 [==============================] - 150s 964ms/step - loss: 0.2586 - accuracy: 0.8912 - val_loss: 0.1735 - val_accuracy: 0.9345
    Epoch 6/50
    156/156 [==============================] - 149s 956ms/step - loss: 0.2536 - accuracy: 0.8927 - val_loss: 0.1659 - val_accuracy: 0.9373
    Epoch 7/50
    156/156 [==============================] - 150s 958ms/step - loss: 0.2546 - accuracy: 0.8919 - val_loss: 0.1667 - val_accuracy: 0.9335
    Epoch 8/50
    156/156 [==============================] - 150s 960ms/step - loss: 0.2493 - accuracy: 0.8954 - val_loss: 0.1771 - val_accuracy: 0.9351
    Epoch 9/50
    156/156 [==============================] - 150s 960ms/step - loss: 0.2510 - accuracy: 0.8967 - val_loss: 0.1655 - val_accuracy: 0.9349
    Epoch 10/50
    156/156 [==============================] - 150s 958ms/step - loss: 0.2456 - accuracy: 0.8975 - val_loss: 0.1669 - val_accuracy: 0.9327
    Epoch 11/50
    156/156 [==============================] - 149s 957ms/step - loss: 0.2531 - accuracy: 0.8934 - val_loss: 0.1648 - val_accuracy: 0.9357
    Epoch 12/50
    156/156 [==============================] - 150s 958ms/step - loss: 0.2449 - accuracy: 0.8992 - val_loss: 0.1651 - val_accuracy: 0.9357
    Epoch 13/50
    156/156 [==============================] - 149s 955ms/step - loss: 0.2460 - accuracy: 0.8972 - val_loss: 0.1654 - val_accuracy: 0.9353
    Epoch 14/50
    156/156 [==============================] - 150s 959ms/step - loss: 0.2419 - accuracy: 0.8980 - val_loss: 0.1706 - val_accuracy: 0.9339
    Epoch 15/50
    156/156 [==============================] - 150s 959ms/step - loss: 0.2416 - accuracy: 0.8982 - val_loss: 0.1613 - val_accuracy: 0.9383
    Epoch 16/50
    156/156 [==============================] - 149s 957ms/step - loss: 0.2422 - accuracy: 0.8973 - val_loss: 0.1626 - val_accuracy: 0.9355
    Epoch 17/50
    156/156 [==============================] - 149s 955ms/step - loss: 0.2370 - accuracy: 0.9024 - val_loss: 0.1653 - val_accuracy: 0.9341
    Epoch 18/50
    156/156 [==============================] - 149s 956ms/step - loss: 0.2300 - accuracy: 0.9065 - val_loss: 0.1671 - val_accuracy: 0.9337
    Epoch 19/50
    156/156 [==============================] - 149s 958ms/step - loss: 0.2345 - accuracy: 0.9034 - val_loss: 0.1641 - val_accuracy: 0.9343
    Epoch 20/50
    156/156 [==============================] - 150s 961ms/step - loss: 0.2326 - accuracy: 0.9045 - val_loss: 0.1631 - val_accuracy: 0.9375
    Epoch 21/50
    156/156 [==============================] - 150s 958ms/step - loss: 0.2371 - accuracy: 0.9004 - val_loss: 0.1616 - val_accuracy: 0.9347
    Epoch 22/50
    156/156 [==============================] - 150s 959ms/step - loss: 0.2307 - accuracy: 0.9022 - val_loss: 0.1618 - val_accuracy: 0.9369
    Epoch 23/50
    156/156 [==============================] - 150s 959ms/step - loss: 0.2308 - accuracy: 0.9038 - val_loss: 0.1565 - val_accuracy: 0.9401
    Epoch 24/50
    156/156 [==============================] - 150s 961ms/step - loss: 0.2332 - accuracy: 0.9031 - val_loss: 0.1554 - val_accuracy: 0.9399
    Epoch 25/50
    156/156 [==============================] - 153s 981ms/step - loss: 0.2344 - accuracy: 0.9051 - val_loss: 0.1606 - val_accuracy: 0.9363
    Epoch 26/50
    156/156 [==============================] - 157s 1s/step - loss: 0.2285 - accuracy: 0.9051 - val_loss: 0.1522 - val_accuracy: 0.9399
    Epoch 27/50
    156/156 [==============================] - 149s 956ms/step - loss: 0.2269 - accuracy: 0.9061 - val_loss: 0.1559 - val_accuracy: 0.9387
    Epoch 28/50
    156/156 [==============================] - 149s 957ms/step - loss: 0.2259 - accuracy: 0.9052 - val_loss: 0.1553 - val_accuracy: 0.9415
    Epoch 29/50
    156/156 [==============================] - 149s 957ms/step - loss: 0.2224 - accuracy: 0.9075 - val_loss: 0.1570 - val_accuracy: 0.9377
    Epoch 30/50
    156/156 [==============================] - 149s 957ms/step - loss: 0.2275 - accuracy: 0.9046 - val_loss: 0.1585 - val_accuracy: 0.9397
    Epoch 31/50
    156/156 [==============================] - 149s 957ms/step - loss: 0.2280 - accuracy: 0.9057 - val_loss: 0.1612 - val_accuracy: 0.9357
    Epoch 32/50
    156/156 [==============================] - 149s 958ms/step - loss: 0.2271 - accuracy: 0.9065 - val_loss: 0.1583 - val_accuracy: 0.9369
    Epoch 33/50
    156/156 [==============================] - 150s 958ms/step - loss: 0.2227 - accuracy: 0.9060 - val_loss: 0.1515 - val_accuracy: 0.9417
    Epoch 34/50
    156/156 [==============================] - 150s 959ms/step - loss: 0.2175 - accuracy: 0.9085 - val_loss: 0.1607 - val_accuracy: 0.9409
    Epoch 35/50
    156/156 [==============================] - 149s 956ms/step - loss: 0.2194 - accuracy: 0.9089 - val_loss: 0.1512 - val_accuracy: 0.9385
    Epoch 36/50
    156/156 [==============================] - 150s 958ms/step - loss: 0.2243 - accuracy: 0.9053 - val_loss: 0.1606 - val_accuracy: 0.9367
    Epoch 37/50
    156/156 [==============================] - 149s 956ms/step - loss: 0.2244 - accuracy: 0.9069 - val_loss: 0.1561 - val_accuracy: 0.9397
    Epoch 38/50
    156/156 [==============================] - 150s 959ms/step - loss: 0.2238 - accuracy: 0.9091 - val_loss: 0.1529 - val_accuracy: 0.9419
    Epoch 39/50
    156/156 [==============================] - 149s 956ms/step - loss: 0.2194 - accuracy: 0.9097 - val_loss: 0.1537 - val_accuracy: 0.9399
    Epoch 40/50
    156/156 [==============================] - 149s 957ms/step - loss: 0.2150 - accuracy: 0.9128 - val_loss: 0.1564 - val_accuracy: 0.9401
    Epoch 41/50
    156/156 [==============================] - 150s 958ms/step - loss: 0.2188 - accuracy: 0.9078 - val_loss: 0.1612 - val_accuracy: 0.9377
    Epoch 42/50
    156/156 [==============================] - 149s 957ms/step - loss: 0.2241 - accuracy: 0.9058 - val_loss: 0.1499 - val_accuracy: 0.9409
    Epoch 43/50
    156/156 [==============================] - 149s 955ms/step - loss: 0.2142 - accuracy: 0.9118 - val_loss: 0.1542 - val_accuracy: 0.9405
    Epoch 44/50
    156/156 [==============================] - 150s 959ms/step - loss: 0.2174 - accuracy: 0.9111 - val_loss: 0.1530 - val_accuracy: 0.9391
    Epoch 45/50
    156/156 [==============================] - 151s 966ms/step - loss: 0.2134 - accuracy: 0.9120 - val_loss: 0.1602 - val_accuracy: 0.9391
    Epoch 46/50
    156/156 [==============================] - 149s 957ms/step - loss: 0.2159 - accuracy: 0.9109 - val_loss: 0.1531 - val_accuracy: 0.9373
    Epoch 47/50
    156/156 [==============================] - 150s 959ms/step - loss: 0.2161 - accuracy: 0.9110 - val_loss: 0.1545 - val_accuracy: 0.9373
    Epoch 48/50
    156/156 [==============================] - 149s 956ms/step - loss: 0.2171 - accuracy: 0.9117 - val_loss: 0.1559 - val_accuracy: 0.9427
    Epoch 49/50
    156/156 [==============================] - 150s 960ms/step - loss: 0.2074 - accuracy: 0.9131 - val_loss: 0.1537 - val_accuracy: 0.9413
    Epoch 50/50
    156/156 [==============================] - 149s 957ms/step - loss: 0.2110 - accuracy: 0.9142 - val_loss: 0.1528 - val_accuracy: 0.9395
    


    
![png](/images/cnn_image/output_21_1.png)
    



    
![png](/images/cnn_image/output_21_2.png)
    


In additional to the model trained on top of InceptionV3, we shall train another CNN without the use of transfer learning


```python
# CNN without InceptionV3
model2 = Sequential()

model2.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(218, 178, 3)))
model2.add(MaxPool2D(pool_size=(2,2)))

model2.add(Conv2D(64, kernel_size=(3,3), activation='relu'))
model2.add(MaxPool2D(pool_size=(2,2)))
model2.add(Dropout(0.25))

model2.add(Conv2D(128, kernel_size=(3,3), activation='relu'))
model2.add(MaxPool2D(pool_size=(2,2)))
model2.add(Dropout(0.25))

model2.add(Flatten())

model2.add(Dense(256, activation='relu'))
model2.add(Dropout(0.4))
model2.add(Dense(128, activation='relu'))
model2.add(Dropout(0.3))

model2.add(Dense(1, activation='sigmoid'))
```


```python
model2.summary()
```

    Model: "sequential_1"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    conv2d_94 (Conv2D)           (None, 216, 176, 32)      896       
    _________________________________________________________________
    max_pooling2d_4 (MaxPooling2 (None, 108, 88, 32)       0         
    _________________________________________________________________
    conv2d_95 (Conv2D)           (None, 106, 86, 64)       18496     
    _________________________________________________________________
    max_pooling2d_5 (MaxPooling2 (None, 53, 43, 64)        0         
    _________________________________________________________________
    dropout_1 (Dropout)          (None, 53, 43, 64)        0         
    _________________________________________________________________
    conv2d_96 (Conv2D)           (None, 51, 41, 128)       73856     
    _________________________________________________________________
    max_pooling2d_6 (MaxPooling2 (None, 25, 20, 128)       0         
    _________________________________________________________________
    dropout_2 (Dropout)          (None, 25, 20, 128)       0         
    _________________________________________________________________
    flatten_1 (Flatten)          (None, 64000)             0         
    _________________________________________________________________
    dense_3 (Dense)              (None, 256)               16384256  
    _________________________________________________________________
    dropout_3 (Dropout)          (None, 256)               0         
    _________________________________________________________________
    dense_4 (Dense)              (None, 128)               32896     
    _________________________________________________________________
    dropout_4 (Dropout)          (None, 128)               0         
    _________________________________________________________________
    dense_5 (Dense)              (None, 1)                 129       
    =================================================================
    Total params: 16,510,529
    Trainable params: 16,510,529
    Non-trainable params: 0
    _________________________________________________________________
    


```python
# Compile the model
model2.compile(optimizer = 'adam',
              loss = 'binary_crossentropy',
              metrics = ['accuracy'])
```


```python
# Training the model
checkpointer_male2 = ModelCheckpoint(filepath = 'saved_models\\weights.best.cnn.male.hdf5')

hist_male2 = model2.fit(train_generator,
                     steps_per_epoch = len(train_generator.filenames)//BATCH_SIZE,
                     epochs = EPOCHS,
                     validation_data = valid_generator,
                     validation_steps = len(valid_generator.filenames)//BATCH_SIZE,
                     callbacks = [checkpointer_male2]
                     )

# Plot loss function value through epochs
plt.figure(figsize=(12, 4))
plt.plot(hist_male2.history['loss'], label = 'training')
plt.plot(hist_male2.history['val_loss'], label = 'validation')
plt.legend()
plt.title('Loss')
plt.show()


# Plot accuracy through epochs
plt.figure(figsize=(12, 4))
plt.plot(hist_male2.history['accuracy'], label = 'training')
plt.plot(hist_male2.history['val_accuracy'], label = 'validation')
plt.legend()
plt.title('Accuracy')
plt.show()
```

    Epoch 1/50
    156/156 [==============================] - 148s 928ms/step - loss: 0.6660 - accuracy: 0.6121 - val_loss: 0.5486 - val_accuracy: 0.7220
    Epoch 2/50
    156/156 [==============================] - 145s 928ms/step - loss: 0.5995 - accuracy: 0.6756 - val_loss: 0.4902 - val_accuracy: 0.7810
    Epoch 3/50
    156/156 [==============================] - 145s 928ms/step - loss: 0.5710 - accuracy: 0.7023 - val_loss: 0.4241 - val_accuracy: 0.8079
    Epoch 4/50
    156/156 [==============================] - 145s 929ms/step - loss: 0.5546 - accuracy: 0.7159 - val_loss: 0.4415 - val_accuracy: 0.8115
    Epoch 5/50
    156/156 [==============================] - 145s 929ms/step - loss: 0.5361 - accuracy: 0.7310 - val_loss: 0.4484 - val_accuracy: 0.7929
    Epoch 6/50
    156/156 [==============================] - 145s 930ms/step - loss: 0.5159 - accuracy: 0.7449 - val_loss: 0.3847 - val_accuracy: 0.8217
    Epoch 7/50
    156/156 [==============================] - 145s 931ms/step - loss: 0.5020 - accuracy: 0.7548 - val_loss: 0.3719 - val_accuracy: 0.8407
    Epoch 8/50
    156/156 [==============================] - 145s 930ms/step - loss: 0.4876 - accuracy: 0.7663 - val_loss: 0.3667 - val_accuracy: 0.8440
    Epoch 9/50
    156/156 [==============================] - 146s 933ms/step - loss: 0.4666 - accuracy: 0.7775 - val_loss: 0.3184 - val_accuracy: 0.8642
    Epoch 10/50
    156/156 [==============================] - 144s 925ms/step - loss: 0.4499 - accuracy: 0.7898 - val_loss: 0.3251 - val_accuracy: 0.8614
    Epoch 11/50
    156/156 [==============================] - 145s 929ms/step - loss: 0.4273 - accuracy: 0.8065 - val_loss: 0.3067 - val_accuracy: 0.8700
    Epoch 12/50
    156/156 [==============================] - 145s 926ms/step - loss: 0.4133 - accuracy: 0.8130 - val_loss: 0.2771 - val_accuracy: 0.8820
    Epoch 13/50
    156/156 [==============================] - 144s 926ms/step - loss: 0.4039 - accuracy: 0.8172 - val_loss: 0.2637 - val_accuracy: 0.8964
    Epoch 14/50
    156/156 [==============================] - 145s 926ms/step - loss: 0.4000 - accuracy: 0.8199 - val_loss: 0.2677 - val_accuracy: 0.8844
    Epoch 15/50
    156/156 [==============================] - 145s 928ms/step - loss: 0.3640 - accuracy: 0.8388 - val_loss: 0.2386 - val_accuracy: 0.8998
    Epoch 16/50
    156/156 [==============================] - 145s 928ms/step - loss: 0.3557 - accuracy: 0.8440 - val_loss: 0.2173 - val_accuracy: 0.9133
    Epoch 17/50
    156/156 [==============================] - 145s 926ms/step - loss: 0.3465 - accuracy: 0.8492 - val_loss: 0.2151 - val_accuracy: 0.9165
    Epoch 18/50
    156/156 [==============================] - 145s 931ms/step - loss: 0.3415 - accuracy: 0.8530 - val_loss: 0.1929 - val_accuracy: 0.9235
    Epoch 19/50
    156/156 [==============================] - 144s 924ms/step - loss: 0.3272 - accuracy: 0.8583 - val_loss: 0.1888 - val_accuracy: 0.9229
    Epoch 20/50
    156/156 [==============================] - 145s 928ms/step - loss: 0.3077 - accuracy: 0.8679 - val_loss: 0.1818 - val_accuracy: 0.9285
    Epoch 21/50
    156/156 [==============================] - 145s 927ms/step - loss: 0.3018 - accuracy: 0.8708 - val_loss: 0.1862 - val_accuracy: 0.9243
    Epoch 22/50
    156/156 [==============================] - 145s 927ms/step - loss: 0.2839 - accuracy: 0.8805 - val_loss: 0.1892 - val_accuracy: 0.9207
    Epoch 23/50
    156/156 [==============================] - 145s 928ms/step - loss: 0.2851 - accuracy: 0.8783 - val_loss: 0.1635 - val_accuracy: 0.9353
    Epoch 24/50
    156/156 [==============================] - 145s 926ms/step - loss: 0.2731 - accuracy: 0.8861 - val_loss: 0.1652 - val_accuracy: 0.9353
    Epoch 25/50
    156/156 [==============================] - 145s 929ms/step - loss: 0.2669 - accuracy: 0.8867 - val_loss: 0.1625 - val_accuracy: 0.9355
    Epoch 26/50
    156/156 [==============================] - 145s 928ms/step - loss: 0.2661 - accuracy: 0.8865 - val_loss: 0.1528 - val_accuracy: 0.9399
    Epoch 27/50
    156/156 [==============================] - 144s 925ms/step - loss: 0.2600 - accuracy: 0.8929 - val_loss: 0.1439 - val_accuracy: 0.9471
    Epoch 28/50
    156/156 [==============================] - 145s 929ms/step - loss: 0.2524 - accuracy: 0.8928 - val_loss: 0.1356 - val_accuracy: 0.9469
    Epoch 29/50
    156/156 [==============================] - 144s 925ms/step - loss: 0.2535 - accuracy: 0.8946 - val_loss: 0.1395 - val_accuracy: 0.9471
    Epoch 30/50
    156/156 [==============================] - 144s 923ms/step - loss: 0.2463 - accuracy: 0.8983 - val_loss: 0.1507 - val_accuracy: 0.9427
    Epoch 31/50
    156/156 [==============================] - 144s 925ms/step - loss: 0.2422 - accuracy: 0.8985 - val_loss: 0.1321 - val_accuracy: 0.9495
    Epoch 32/50
    156/156 [==============================] - 145s 930ms/step - loss: 0.2467 - accuracy: 0.8956 - val_loss: 0.1399 - val_accuracy: 0.9481
    Epoch 33/50
    156/156 [==============================] - 145s 929ms/step - loss: 0.2430 - accuracy: 0.8985 - val_loss: 0.1254 - val_accuracy: 0.9535
    Epoch 34/50
    156/156 [==============================] - 145s 926ms/step - loss: 0.2416 - accuracy: 0.8995 - val_loss: 0.1284 - val_accuracy: 0.9527
    Epoch 35/50
    156/156 [==============================] - 145s 928ms/step - loss: 0.2344 - accuracy: 0.9025 - val_loss: 0.1341 - val_accuracy: 0.9493
    Epoch 36/50
    156/156 [==============================] - 145s 928ms/step - loss: 0.2329 - accuracy: 0.9013 - val_loss: 0.1318 - val_accuracy: 0.9471
    Epoch 37/50
    156/156 [==============================] - 145s 926ms/step - loss: 0.2358 - accuracy: 0.9019 - val_loss: 0.1474 - val_accuracy: 0.9405
    Epoch 38/50
    156/156 [==============================] - 145s 927ms/step - loss: 0.2346 - accuracy: 0.9041 - val_loss: 0.1299 - val_accuracy: 0.9489
    Epoch 39/50
    156/156 [==============================] - 145s 926ms/step - loss: 0.2335 - accuracy: 0.9033 - val_loss: 0.1240 - val_accuracy: 0.9503
    Epoch 40/50
    156/156 [==============================] - 145s 928ms/step - loss: 0.2307 - accuracy: 0.9062 - val_loss: 0.1458 - val_accuracy: 0.9403
    Epoch 41/50
    156/156 [==============================] - 145s 926ms/step - loss: 0.2242 - accuracy: 0.9069 - val_loss: 0.1254 - val_accuracy: 0.9491
    Epoch 42/50
    156/156 [==============================] - 145s 927ms/step - loss: 0.2198 - accuracy: 0.9112 - val_loss: 0.1214 - val_accuracy: 0.9509
    Epoch 43/50
    156/156 [==============================] - 145s 931ms/step - loss: 0.2203 - accuracy: 0.9083 - val_loss: 0.1397 - val_accuracy: 0.9451
    Epoch 44/50
    156/156 [==============================] - 145s 928ms/step - loss: 0.2209 - accuracy: 0.9072 - val_loss: 0.1270 - val_accuracy: 0.9505
    Epoch 45/50
    156/156 [==============================] - 145s 926ms/step - loss: 0.2281 - accuracy: 0.9073 - val_loss: 0.1298 - val_accuracy: 0.9529
    Epoch 46/50
    156/156 [==============================] - 145s 927ms/step - loss: 0.2162 - accuracy: 0.9106 - val_loss: 0.1254 - val_accuracy: 0.9529
    Epoch 47/50
    156/156 [==============================] - 145s 928ms/step - loss: 0.2151 - accuracy: 0.9092 - val_loss: 0.1248 - val_accuracy: 0.9515
    Epoch 48/50
    156/156 [==============================] - 145s 927ms/step - loss: 0.2077 - accuracy: 0.9132 - val_loss: 0.1223 - val_accuracy: 0.9525
    Epoch 49/50
    156/156 [==============================] - 145s 930ms/step - loss: 0.2173 - accuracy: 0.9106 - val_loss: 0.1215 - val_accuracy: 0.9519
    Epoch 50/50
    156/156 [==============================] - 145s 929ms/step - loss: 0.2105 - accuracy: 0.9118 - val_loss: 0.1223 - val_accuracy: 0.9543
    


    
![png](/images/cnn_image/output_26_1.png)
    



    
![png](/images/cnn_image/output_26_2.png)
    


<a id = '4.3'></a>
### 4.3 Model Evaluation
Now that the models are trained, we will evaluate them by predicting the classes in the test set.

#### InceptionV3 model


```python
# Make predictions on test set
pred = model.predict(test_generator,
                      steps = len(test_generator),
                      verbose = 1)

filenames = test_generator.filenames

results = pd.DataFrame(data = {'image_id':filenames, 'prob':pred[:,0]})
results.image_id = results.image_id.str.split('\\').str[1]
results.head()
```

    40/40 [==============================] - 33s 826ms/step
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>image_id</th>
      <th>prob</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>182641.jpg</td>
      <td>0.998044</td>
    </tr>
    <tr>
      <th>1</th>
      <td>182653.jpg</td>
      <td>0.194666</td>
    </tr>
    <tr>
      <th>2</th>
      <td>182657.jpg</td>
      <td>0.997294</td>
    </tr>
    <tr>
      <th>3</th>
      <td>182662.jpg</td>
      <td>0.014657</td>
    </tr>
    <tr>
      <th>4</th>
      <td>182663.jpg</td>
      <td>0.937032</td>
    </tr>
  </tbody>
</table>
</div>




```python
# Create dataframe containing the test set, predictions, and actual class
results['class'] = results['prob']
results['class'] = results['class'].apply(lambda x: 1 if x <= 0.5 else 0)
results = pd.merge(results, df_attr[['image_id', 'Male']], how = 'left', on = 'image_id')

print(len(results))
print(results.head())
```

    5000
         image_id      prob  class  Male
    0  182641.jpg  0.998044      0     0
    1  182653.jpg  0.194666      1     1
    2  182657.jpg  0.997294      0     0
    3  182662.jpg  0.014657      1     1
    4  182663.jpg  0.937032      0     0
    


```python
print("Model accuracy score: ", accuracy_score(results['Male'], results['class']))
print("Model f1 score: ", f1_score(results['Male'], results['class']))
```

    Model accuracy score:  0.9244
    Model f1 score:  0.9249404289118347
    

The model has managed to achieve over 92% classification accuracy when classifying where an image is "Male" or not. 

#### CNN without using InceptionV3


```python
# Make predictions on test set
pred2 = model2.predict(test_generator,
                      steps = len(test_generator),
                      verbose = 1)

filenames = test_generator.filenames

results2 = pd.DataFrame(data = {'image_id':filenames, 'prob':pred2[:,0]})
results2.image_id = results2.image_id.str.split('\\').str[1]
results2.head()
```

    40/40 [==============================] - 4s 103ms/step
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>image_id</th>
      <th>prob</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>182641.jpg</td>
      <td>0.999999</td>
    </tr>
    <tr>
      <th>1</th>
      <td>182653.jpg</td>
      <td>0.092155</td>
    </tr>
    <tr>
      <th>2</th>
      <td>182657.jpg</td>
      <td>0.893312</td>
    </tr>
    <tr>
      <th>3</th>
      <td>182662.jpg</td>
      <td>0.085085</td>
    </tr>
    <tr>
      <th>4</th>
      <td>182663.jpg</td>
      <td>0.999456</td>
    </tr>
  </tbody>
</table>
</div>




```python
# Create dataframe containing the test set, predictions, and actual class
results2['class'] = results2['prob']
results2['class'] = results2['class'].apply(lambda x: 1 if x <= 0.5 else 0)
results2 = pd.merge(results2, df_attr[['image_id', 'Male']], how = 'left', on = 'image_id')

print(len(results2))
print(results2.head())
```

    5000
         image_id      prob  class  Male
    0  182641.jpg  0.999999      0     0
    1  182653.jpg  0.092155      1     1
    2  182657.jpg  0.893312      0     0
    3  182662.jpg  0.085085      1     1
    4  182663.jpg  0.999456      0     0
    


```python
print("Model accuracy score: ", accuracy_score(results2['Male'], results2['class']))
print("Model f1 score: ", f1_score(results2['Male'], results2['class']))
```

    Model accuracy score:  0.9418
    Model f1 score:  0.9410334346504559
    

In comparison, the traditional CNN managed to obtain an accuracy score of over 94% which is higher than the InceptionV3 model. As both models took a similar length of time to train, the traditional CNN is the better performance in this case. 

Something that can be done differently for the InceptionV3 model would be to train the top layers of the InceptionV3 model in addition to our custom layers, instead of training only the custom layers. This approach may lead to a better performing model.


<a id='4.4'></a>
### 4.4 *Examples*
Finally, we shall look at some of the predictions from the InceptionV3 model and see examples of correct and incorrect predictions.


```python
# Function to plot image and get correct probability and class
def get_classes(row, j):
    prob = row['prob']*100
    
    
    EXAMPLE_PIC = test_folder + 'test_Male\\' + str(row['image_id'])
    img = load_img(EXAMPLE_PIC)
    ax[j].grid(False)
    ax[j].imshow(img)
    
    
    if prob <= 50:
        ax[j].set_title("Male, Prob: " + "{:.2f}".format(100 - prob) + "%")
        
    else:
        ax[j].set_title('Female, Prob: ' + "{:.2f}".format(prob) + "%")
        
```


```python
# Correct predictions
df_correct = results.loc[results.Male == results['class']]
correct = df_correct.sample(3, random_state = 3)

# Plot correct predictions
fig, ax = plt.subplots(1,3, figsize = (12,6))
j = 0
for i, row in correct.iterrows():
    get_classes(row, j)
    j += 1
```


    
![png](/images/cnn_image/output_37_0.png)
    



```python
# Incorrect predictions
df_incorrect = results.loc[results.Male != results['class']]
incorrect = df_incorrect.sample(3, random_state = 3)

# Plot incorrect predictions
fig, ax = plt.subplots(1,3, figsize = (12,6))
j = 0
for i, row in incorrect.iterrows():
    get_classes(row, j)
    j += 1
```


    
![png](/images/cnn_image/output_38_0.png)
    


From the incorrect predictions we observe that the model is confidently wrong in some cases, and the images may suggest that someone wearing large accessories on their faces while having no obvious "Male" feature like a moustache may end up confusing the model.

***
<a id='5'></a>
## 5. Conclusion
In this notebook we have created both a traditional CNN and a CNN built on InceptionV3 to classify images from the CelebA dataset by their gender, achieving a good accuracy score of over 90% in both models.
